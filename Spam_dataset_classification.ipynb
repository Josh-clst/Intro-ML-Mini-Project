{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb9c1dc",
   "metadata": {},
   "source": [
    "# Spam Dataset Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a55442-ef40-4b01-9d81-aed2dd75e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from functions import scaled_tensorize_data, datasets_and_loaders, train_model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f4777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>...</th>\n",
       "      <th>col_48</th>\n",
       "      <th>col_49</th>\n",
       "      <th>col_50</th>\n",
       "      <th>col_51</th>\n",
       "      <th>col_52</th>\n",
       "      <th>col_53</th>\n",
       "      <th>col_54</th>\n",
       "      <th>col_55</th>\n",
       "      <th>col_56</th>\n",
       "      <th>col_57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7  col_8  col_9  ...  \\\n",
       "0   0.00   0.64   0.64    0.0   0.32   0.00   0.00   0.00   0.00   0.00  ...   \n",
       "1   0.21   0.28   0.50    0.0   0.14   0.28   0.21   0.07   0.00   0.94  ...   \n",
       "2   0.06   0.00   0.71    0.0   1.23   0.19   0.19   0.12   0.64   0.25  ...   \n",
       "3   0.00   0.00   0.00    0.0   0.63   0.00   0.31   0.63   0.31   0.63  ...   \n",
       "4   0.00   0.00   0.00    0.0   0.63   0.00   0.31   0.63   0.31   0.63  ...   \n",
       "\n",
       "   col_48  col_49  col_50  col_51  col_52  col_53  col_54  col_55  col_56  \\\n",
       "0    0.00   0.000     0.0   0.778   0.000   0.000   3.756      61     278   \n",
       "1    0.00   0.132     0.0   0.372   0.180   0.048   5.114     101    1028   \n",
       "2    0.01   0.143     0.0   0.276   0.184   0.010   9.821     485    2259   \n",
       "3    0.00   0.137     0.0   0.137   0.000   0.000   3.537      40     191   \n",
       "4    0.00   0.135     0.0   0.135   0.000   0.000   3.537      40     191   \n",
       "\n",
       "   col_57  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"DATASET\") / \"spambase.data\"\n",
    "df = pd.read_csv(data_path, header=None)\n",
    "df.columns = [f\"col_{i}\" for i in range(df.shape[1])]\n",
    "display(df.head())\n",
    "\n",
    "print(df.shape)      # (n_lignes, n_colonnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18646b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "col_0     0\n",
       "col_43    0\n",
       "col_31    0\n",
       "col_32    0\n",
       "col_33    0\n",
       "col_34    0\n",
       "col_35    0\n",
       "col_36    0\n",
       "col_37    0\n",
       "col_38    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# total de NaN dans tout le DataFrame\n",
    "total_nan = int(df.isna().sum().sum())\n",
    "print(\"Total NaN:\", total_nan)\n",
    "\n",
    "# NaN par colonne (ordonné)\n",
    "nan_by_col = df.isna().sum().sort_values(ascending=False)\n",
    "display(nan_by_col.head(10))  # top 10 colonnes avec le plus de NaN\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3de880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu groupes: [['col_0'], ['col_1'], ['col_2'], ['col_3'], ['col_4']]\n",
      "Score test: 0.9294245385450597\n",
      "VarianceThreshold F1: 0.9023395347207346 +/- 0.009077532224183586\n",
      "Baseline  Acc=0.929  F1=0.909\n",
      "       group  size  acc_drop   f1_drop\n",
      "0  (col_24,)     1  0.037134  0.051113\n",
      "1  (col_15,)     1  0.027904  0.037768\n",
      "2  (col_26,)     1  0.021824  0.031663\n",
      "3  (col_45,)     1  0.020521  0.027882\n",
      "4  (col_52,)     1  0.022910  0.027189\n",
      "5   (col_6,)     1  0.020955  0.026447\n",
      "6  (col_55,)     1  0.017481  0.021252\n",
      "7  (col_25,)     1  0.012704  0.018922\n",
      "8  (col_22,)     1  0.014984  0.017570\n",
      "9  (col_28,)     1  0.008903  0.012802\n",
      "Score test (groupe shufflé): 0.9283387622149837\n",
      "Nb features gardées: 8\n",
      "Score test (sélection): 0.8914223669923995\n",
      "F1 CV (sélection): 0.8576121095324428 +/- 0.007335637180195787\n"
     ]
    }
   ],
   "source": [
    "# --- 1) pipe_var minimaliste (comme demandé) ---\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1 = make_scorer(f1_score)\n",
    "\n",
    "def pipe_var(X_tr, y_tr, X_te, y_te, thrs, X, y):\n",
    "    pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"var\", VarianceThreshold(threshold=thrs)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    print(\"Score test:\", pipe.score(X_te, y_te))\n",
    "\n",
    "    # IMPORTANT: on passe l'ESTIMATEUR (pipe), pas la fonction pipe_var\n",
    "    scores = cross_val_score(pipe, X, y, cv=cv, scoring=f1, n_jobs=-1)\n",
    "    print(\"VarianceThreshold F1:\", scores.mean(), \"+/-\", scores.std())\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# --- 2) Corrélation: regrouper + shuffler un groupe ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def corr_groups(X, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Groupes de colonnes corrélées (|corr| >= threshold) via composantes connexes.\n",
    "    Retourne: list[list[str]]\n",
    "    \"\"\"\n",
    "    C = X.corr().abs()\n",
    "    cols = list(X.columns)\n",
    "    seen, groups = set(), []\n",
    "    for c in cols:\n",
    "        if c in seen:\n",
    "            continue\n",
    "        grp, stack = [], [c]\n",
    "        seen.add(c)\n",
    "        while stack:\n",
    "            u = stack.pop()\n",
    "            grp.append(u)\n",
    "            neigh = C.index[(C[u] >= threshold) & (C.index != u)]\n",
    "            for v in neigh:\n",
    "                if v not in seen:\n",
    "                    seen.add(v)\n",
    "                    stack.append(v)\n",
    "        groups.append(sorted(grp))\n",
    "    return groups\n",
    "\n",
    "def shuffle_group(X, group, random_state=42):\n",
    "    \"\"\"\n",
    "    Retourne une copie de X où TOUTES les colonnes du groupe sont shufflées\n",
    "    avec la MÊME permutation (on casse le lien de ce groupe avec y).\n",
    "    \"\"\"\n",
    "    Xp = X.copy()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    perm = rng.permutation(len(Xp))\n",
    "    for c in group:\n",
    "        Xp[c] = Xp[c].to_numpy()[perm]\n",
    "    return Xp\n",
    "\n",
    "\n",
    "# helper pour évaluer l’impact moyen d’un groupe sur accuracy et F1\n",
    "def impact_par_groupe(pipe, X_te, y_te, groups, n_repeats=10, seed=0):\n",
    "    base_acc = accuracy_score(y_te, pipe.predict(X_te))\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        p = pipe.predict_proba(X_te)[:, 1]\n",
    "        base_f1 = f1_score(y_te, (p >= 0.5).astype(int))\n",
    "    else:\n",
    "        base_f1 = f1_score(y_te, pipe.predict(X_te))\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    res = []\n",
    "    for g in groups:\n",
    "        acc_drops, f1_drops = [], []\n",
    "        for _ in range(n_repeats):\n",
    "            X_te_shuf = shuffle_group(X_te, g, random_state=int(rng.integers(0, 1e9)))\n",
    "            if hasattr(pipe, \"predict_proba\"):\n",
    "                p = pipe.predict_proba(X_te_shuf)[:, 1]\n",
    "                yhat = (p >= 0.5).astype(int)\n",
    "            else:\n",
    "                yhat = pipe.predict(X_te_shuf)\n",
    "            acc_drops.append(base_acc - accuracy_score(y_te, yhat))\n",
    "            f1_drops.append(base_f1 - f1_score(y_te, yhat))\n",
    "        res.append({\"group\": tuple(g), \"size\": len(g),\n",
    "                    \"acc_drop\": float(np.mean(acc_drops)),\n",
    "                    \"f1_drop\": float(np.mean(f1_drops))})\n",
    "    df_imp = pd.DataFrame(res).sort_values([\"f1_drop\",\"acc_drop\"], ascending=False).reset_index(drop=True)\n",
    "    print(f\"Baseline  Acc={base_acc:.3f}  F1={base_f1:.3f}\")\n",
    "    return df_imp\n",
    "\n",
    "\n",
    "target = df.columns[-1]\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 1) Groupes corrélés\n",
    "groups = corr_groups(X_tr, threshold=0.9)\n",
    "print(\"Aperçu groupes:\", groups[:5])\n",
    "\n",
    "# 2) Entraînement + CV\n",
    "pipe = pipe_var(X_tr, y_tr, X_te, y_te, 1e-5, X, y)\n",
    "\n",
    "# 3) Impact par groupe (moyenne sur quelques shuffles)\n",
    "impacts = impact_par_groupe(pipe, X_te, y_te, groups, n_repeats=10)\n",
    "print(impacts.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4) Test rapide d’un groupe particulier \n",
    "\n",
    "g0 = groups[2]\n",
    "X_te_shuf = shuffle_group(X_te, g0, random_state=0)\n",
    "print(\"Score test (groupe shufflé):\", pipe.score(X_te_shuf, y_te))\n",
    "\n",
    "\n",
    "#On garde les groupes les plus important\n",
    "K = 8 \n",
    "keep_groups = impacts.head(K)[\"group\"].tolist()\n",
    "keep_features = sorted({f for g in keep_groups for f in g})\n",
    "print(\"Nb features gardées:\", len(keep_features))\n",
    "\n",
    "# Refit sur le sous-ensemble\n",
    "pipe_sel = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "pipe_sel.fit(X_tr[keep_features], y_tr)\n",
    "\n",
    "# Évalue sur test\n",
    "print(\"Score test (sélection):\", pipe_sel.score(X_te[keep_features], y_te))\n",
    "\n",
    "# CV sur tout X,y avec seulement les features gardées\n",
    "scores_sel = cross_val_score(\n",
    "    pipe_sel, X[keep_features], y, cv=cv, scoring=f1, n_jobs=-1\n",
    ")\n",
    "print(\"F1 CV (sélection):\", scores_sel.mean(), \"+/-\", scores_sel.std())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d72ec-2443-49d6-8c9f-23bc9ca47f44",
   "metadata": {},
   "source": [
    "### LDA classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ceabb00-7ed8-4e88-a259-82567b38f850",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(2940     True\n1303    False\n3468     True\n3181     True\n794     False\n        ...  \n1861     True\n2366     True\n330     False\n536     False\n3114     True\nName: col_57, Length: 3680, dtype: bool, slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.miniconda\\envs\\LAB_3.11\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:173\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: '(2940     True\n1303    False\n3468     True\n3181     True\n794     False\n        ...  \n1861     True\n2366     True\n330     False\n536     False\n3114     True\nName: col_57, Length: 3680, dtype: bool, slice(None, None, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mInvalidIndexError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# classification\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m class_means,class_cov = \u001b[43mfunctions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLDA_classifier_train_cov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m predicted_labels = functions.LDA_classifier_predict_cov(X_te,class_means,class_cov,\u001b[32m2\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m\u001b[33m means :\u001b[39m\u001b[33m\"\u001b[39m, class_means)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\TPs_______________________________________________________\\Intro-ML-Mini-Project\\functions.py:29\u001b[39m, in \u001b[36mLDA_classifier_train_cov\u001b[39m\u001b[34m(training_data, training_labels, nb_classes)\u001b[39m\n\u001b[32m     27\u001b[39m class_cov = np.zeros((nb_classes,training_data.shape[\u001b[32m1\u001b[39m],training_data.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_classes):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     class_i_data = \u001b[43mtraining_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m         \n\u001b[32m     30\u001b[39m     class_means[i,:] = np.mean(class_i_data,axis = \u001b[32m0\u001b[39m)  \n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 2. Compute pooled covariance (shared covariance)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.miniconda\\envs\\LAB_3.11\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.miniconda\\envs\\LAB_3.11\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3824\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3824\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3825\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.miniconda\\envs\\LAB_3.11\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6072\u001b[39m, in \u001b[36mIndex._check_indexing_error\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   6068\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m   6069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[32m   6070\u001b[39m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[32m   6071\u001b[39m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6072\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[31mInvalidIndexError\u001b[39m: (2940     True\n1303    False\n3468     True\n3181     True\n794     False\n        ...  \n1861     True\n2366     True\n330     False\n536     False\n3114     True\nName: col_57, Length: 3680, dtype: bool, slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "class_means,class_cov = functions.LDA_classifier_train_cov(X_tr, y_tr,2)\n",
    "predicted_labels = functions.LDA_classifier_predict_cov(X_te,class_means,class_cov,2)\n",
    "\n",
    "print(\"class' means :\", class_means)\n",
    "print(\"class' covariance :\", class_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9e371-8856-4e06-a3ce-88139eccc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test accuracies\n",
    "training_accuracy, test_accuracy = functions.train_test_accuracy_cov(X_tr,y_tr,X_te,y_te, class_means, class_cov, LDA_classifier_predict_cov)\n",
    "\n",
    "print(\"training_accuracy :\", training_accuracy)\n",
    "print(\"test_accuracy :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fda93-99ed-478a-996d-d1e6c19462ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boundaries\n",
    "grid_size = 10000\n",
    "\n",
    "x = np.linspace(-100,100,grid_size)\n",
    "y = np.linspace(-100,100,grid_size)\n",
    "functions.plot_decision_boundary(x,y,X,Y,class_means,LDA_classifier_predict_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1ab20-e5ce-4886-a32e-e6ade6a4b97f",
   "metadata": {},
   "source": [
    "### QDA classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be630548-246f-4e0a-b475-b045eeb563b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using QDA classifier\n",
    "class_means,cov = functions.QDA_classifier_train(X_tr, y_tr,2)\n",
    "predicted_labels = functions.QDA_classifier_predict(X_te,class_means,cov,2)\n",
    "\n",
    "print(\"class' means : {class_means}\")\n",
    "print(\"class' covariance : {cov}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e7750-c5bd-42c5-acb7-4282a9ff7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test accuracies\n",
    "training_accuracy, test_accuracy = functions.train_test_accuracy_cov(X_tr,y_tr,X_te,y_te, class_means, cov, QDA_classifier_predict)\n",
    "\n",
    "print(\"training_accuracy :\", training_accuracy)\n",
    "print(\"test_accuracy :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511e48f-cd8b-48cf-a726-39f7c7778282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boundaries\n",
    "grid_size = 10000\n",
    "\n",
    "x = np.linspace(-100,100,grid_size)\n",
    "y = np.linspace(-100,100,grid_size)\n",
    "functions.plot_decision_boundary_cov(x,y,X,labels,class_means,QDA_classifier_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
